{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from postprocess import ResultProcess, Eval\n",
    "import pandas as pd \n",
    "from const import Const as C\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"News\" #\"Movie\" #\"Book\"  Music Movie  News\n",
    "result_folder=\"llama_result\"\n",
    "base_folder=f\"./{result_folder}/{dataset_name}\"\n",
    "from get_transition_mat_70b import get_transition_mat\n",
    "folder_path=f\"{base_folder}/{dataset_name}_train\"\n",
    "num_options = 5 # replace with the number of options\n",
    "trans_mat = get_transition_mat(folder_path, num_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_in_mat_sty(transitions):\n",
    "    import numpy as np\n",
    "    # Order of keys\n",
    "    keys = sorted(transitions.keys())\n",
    "    # Create 5x5 matrix\n",
    "    matrix = np.zeros((5, 5))\n",
    "    for i, key1 in enumerate(keys):\n",
    "        for j, key2 in enumerate(keys):\n",
    "            matrix[i, j] = transitions[key1][key2]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "total_samples = sum([sum(value.values()) for value in trans_mat.values()])\n",
    "each_train_num={k:sum(trans_mat[k].values()) for k in trans_mat.keys()}\n",
    "prior_prob = {k: sum(value.values())/ total_samples for value in trans_mat.values() for k in ['A', 'B', 'C', 'D', 'E']}\n",
    "likelihood_prob = {k: {evi:trans_mat[k][evi]/each_train_num[k] for evi in trans_mat[k].keys() } for k in ['A', 'B', 'C', 'D', 'E']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = sum([sum(value.values()) for value in trans_mat.values()])\n",
    "each_train_num={k:sum(trans_mat[k].values()) for k in trans_mat.keys()}\n",
    "prior_prob = {k: sum(value.values())/ total_samples for value in trans_mat.values() for k in ['A', 'B', 'C', 'D', 'E']}\n",
    "likelihood_prob = {k: {evi:trans_mat[k][evi]/each_train_num[k] for evi in trans_mat[k].keys() } for k in ['A', 'B', 'C', 'D', 'E']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Function to load jsonl file into a pandas DataFrame\n",
    "def load_jsonl_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Parse each line as a JSON object and collect into a list\n",
    "        jsonl_content = [json.loads(line) for line in file]\n",
    "    # Convert list of JSON objects to a pandas DataFrame\n",
    "    return pd.DataFrame(jsonl_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_post(evi,prior_prob):\n",
    "    posterior_prob={}\n",
    "\n",
    "    for hpy in trans_mat.keys():\n",
    "        posterior_prob[hpy]=prior_prob[hpy]*likelihood_prob[hpy][evi]\n",
    "    posterior_prob={key:posterior_prob[key]/sum(posterior_prob.values()) for key in posterior_prob.keys()}\n",
    "    return posterior_prob\n",
    "\n",
    "def cal_s(file_path,content_s,content_post,content,ranked_origin_pos,post_ranked_pos,df):\n",
    "    valid_num=0\n",
    "    except_num=0    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            # rank_order = data['response']['gpt_data']['choices'][0]['message']['content']\n",
    "            try:\n",
    "                text = data['response']#['result']#['choices'][0]['message']['content']#['gpt_data']\n",
    "            except:\n",
    "                except_num+=1\n",
    "                continue\n",
    "            match = re.search(r'\"rank_order\":\\s*\"([^\"]*)\"', text, re.DOTALL)\n",
    "            \n",
    "            if not match:\n",
    "                except_num+=1\n",
    "                continue\n",
    "            else:\n",
    "                order = match.group(1)\n",
    "                task_id=data[\"task_id\"]\n",
    "                request_data=df[df['task_id'] == task_id]\n",
    "                origin_pos_line=request_data[\"original_positions_after_shuffle\"].values[0]\n",
    "                order_number = []\n",
    "                for item in order.split(' '):\n",
    "                    if len(item) == 1 and item in ['A','B','C','D','E']:\n",
    "                        order_number.append(ord(item) - ord('A'))\n",
    "                    else:\n",
    "                        continue\n",
    "                        print(f\"item:{item}\")\n",
    "                if len(order_number)>len(origin_pos_line):\n",
    "                    continue\n",
    "                ranked_origin_pos_line=[]\n",
    "                for i in order_number:\n",
    "                    if i<len(origin_pos_line):\n",
    "                        # print(i,'--',len(origin_pos_line))\n",
    "                        ranked_origin_pos_line.append(origin_pos_line[i])\n",
    "                    else:\n",
    "                        print(\"i outOfindex:\",i)\n",
    "                # ranked_origin_pos_line=[origin_pos_line[i] for i in order_number]\n",
    "                if len(ranked_origin_pos_line)==0:\n",
    "                    continue\n",
    "                if task_id not in content_s.keys():\n",
    "                    content_s[task_id]=[]\n",
    "                    content_post[task_id]=[{'A':0.2,'B':0.2,'C':0.2,'D':0.2,'E':0.2}] \n",
    "                    content[task_id]=[]\n",
    "                    ranked_origin_pos[task_id]=[]\n",
    "\n",
    "                    post_ranked_pos[task_id]=[]\n",
    "                    # content_s[task_id].append(sum(-p*math.log(p) for p in content_post[task_id][-1].values()))\n",
    "# \n",
    "                content_post[task_id].append(cal_post(chr(ord('A')+ranked_origin_pos_line[0]),content_post[task_id][-1]))\n",
    "                content_s[task_id].append(sum(-p*math.log(p) if p > 0 else 0 for p in content_post[task_id][-1].values()))\n",
    "                # sum(-p*math.log(p) if p > 0 else 0 for p in content_post[task_id][-1].values())\n",
    "\n",
    "                content[task_id].append(order)\n",
    "                ranked_origin_pos[task_id].append(ranked_origin_pos_line)\n",
    "                \n",
    "                post_p=content_post[task_id][-1]\n",
    "                sorted_data = sorted(post_p.items(), key=lambda x: x[1], reverse=True)\n",
    "                post_ranked_pos[task_id].append([ord(item[0])-ord('A') for item in sorted_data])\n",
    "                valid_num+=1\n",
    "    print(f\"valid_num:{valid_num},except_num:{except_num}\")\n",
    "    return content_s,content_post,content,ranked_origin_pos,post_ranked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_by_e(content_s,ranked_origin_pos,content_post,num=10,save_multi=True):\n",
    "    final_to_conbined={}\n",
    "    def tend_decrease(input:list):\n",
    "        if input[-1]<input[-2] and input[-2]<input[-3]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    no_shoulian=0\n",
    "    extra_cnt=0\n",
    "    record={}\n",
    "    used_e={}\n",
    "    for i in range(3,num):\n",
    "        record[i]=0\n",
    "\n",
    "    for key,val in content_s.items():\n",
    "        key_ranked_pos=ranked_origin_pos[key]\n",
    "        key_num=int(key.split(\"-\")[0])\n",
    "        shoulian=False\n",
    "        for i in range(3,len(val)):\n",
    "            if not tend_decrease(val[i-3:i]):\n",
    "                if not shoulian:\n",
    "                    if not save_multi:\n",
    "                        final_to_conbined[key_num]=key_ranked_pos[i]\n",
    "                    else:\n",
    "                        final_to_conbined[key_num]=key_ranked_pos[i-3:i]\n",
    "\n",
    "                else:\n",
    "                    extra_cnt+=1\n",
    "\n",
    "                    post_p=content_post[key][i]\n",
    "                    sorted_data = sorted(post_p.items(), key=lambda x: x[1], reverse=True)\n",
    "                    if save_multi:\n",
    "                        order=[ord(item[0])-ord('A') for item in sorted_data]\n",
    "                        final_to_conbined[key_num]=[order]\n",
    "                    else:\n",
    "                        final_to_conbined[key_num]=[ord(item[0])-ord('A') for item in sorted_data]\n",
    "                    # print(post_p)\n",
    "                    \n",
    "                record[i]+=1\n",
    "                used_e[key]=i\n",
    "                break\n",
    "            else:\n",
    "                if i==len(val)-1:\n",
    "                    if not save_multi:\n",
    "                        final_to_conbined[key_num]=key_ranked_pos[i]\n",
    "                    else:\n",
    "                        final_to_conbined[key_num]=key_ranked_pos[i-3:i]\n",
    "\n",
    "                    used_e[key]=i\n",
    "\n",
    "                    no_shoulian+=1\n",
    "                    # print(f\"{key} no shoulian {content_s[key]}\")\n",
    "                    record[i]+=1\n",
    "                    break\n",
    "                else:\n",
    "                    i+=1\n",
    "                    shoulian=True\n",
    "                    continue\n",
    "    print(\"no_shoulian num:\",no_shoulian,\"in first 3:\",extra_cnt)\n",
    "    print(\"get value position counts:\",record)\n",
    "    return final_to_conbined,used_e\n",
    "# final_to_conbined=get_by_e(content_s,ranked_origin_pos,content_post,num=10,save_multi=save_multi)#\n",
    "# final_to_conbined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_y_true(data: pd.DataFrame, begin_index: int, end_index: int):\n",
    "    \"\"\"Get ground truth from data.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The Dataframe of data.\n",
    "        begin_index (int): The begin index of data.\n",
    "        end_index (int): The end index of data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, List[int]]: The ground truth from begin index to end index.\n",
    "    \"\"\"\n",
    "    y_true = data[begin_index:end_index].pos_target_index.to_dict()\n",
    "\n",
    "    return y_true\n",
    "\n",
    "# y_pred={}\n",
    "def conbine_by_score(to_conbined):\n",
    "    def score_cal(scores,pred_list):\n",
    "        # scores=[0,0,0,0,0]\n",
    "        # print(scores)\n",
    "        for i in range(len(pred_list)):\n",
    "            # print(scores[pred_list[i]],len(pred_list))\n",
    "            scores[pred_list[i]] += len(pred_list) - i\n",
    "        return scores\n",
    "    y_pred={}\n",
    "    for key,val in to_conbined.items():\n",
    "        scores=[0,0,0,0,0]\n",
    "        for one_pred in val:\n",
    "            # print(one_pred)\n",
    "            scores=score_cal(scores,one_pred)\n",
    "        \n",
    "        idx_score={}\n",
    "        for idx,score0 in enumerate(scores):\n",
    "            idx_score[idx]=score0\n",
    "\n",
    "        sorted_data = sorted(idx_score.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        sorted_idx = [item[0] for item in sorted_data]\n",
    "        y_pred[key]= sorted_idx\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def combine_by_voting(to_combine):\n",
    "    def most_common_in_position(rankings, position):\n",
    "        # Count occurrences of each element at a given position\n",
    "        elements_at_position = [ranking[position] for ranking in rankings if position<len(ranking)]\n",
    "        element_counts = {}\n",
    "        for element in elements_at_position:\n",
    "            if element in element_counts:\n",
    "                element_counts[element] += 1\n",
    "            else:\n",
    "                element_counts[element] = 1\n",
    "        # Get the most common element at the given position\n",
    "        most_common_element = max(element_counts, key=element_counts.get)\n",
    "        return most_common_element\n",
    "    aggregated_rankings = {}\n",
    "    for key, rankings in to_combine.items():\n",
    "        if len(rankings) == 1:\n",
    "            aggregated = rankings\n",
    "        else:\n",
    "            aggregated = [most_common_in_position(rankings, i) for i in range(len(rankings[0]))]\n",
    "\n",
    "        aggregated_rankings[key] = aggregated\n",
    "\n",
    "    return aggregated_rankings\n",
    "\n",
    "def get_from_file(response_path,df):\n",
    "    result = {}\n",
    "    valid_num=0\n",
    "    except_num=0\n",
    "    with open(response_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            # rank_order = data['response']['gpt_data']['choices'][0]['message']['content']\n",
    "            try:\n",
    "                text = data['response']#['result']#['choices'][0]['message']['content']#['gpt_data']\n",
    "            except:\n",
    "                except_num+=1\n",
    "                continue\n",
    "            # match = re.search(r'\\\"rank_order\\\": \\\"(.*?)\\\"\\\\n', text)\n",
    "            match = re.search(r'\"rank_order\":\\s*\"([^\"]*)\"', text, re.DOTALL)\n",
    "            if not match:\n",
    "                except_num+=1\n",
    "                continue\n",
    "            else:\n",
    "                order = match.group(1)\n",
    "                # print('rank_order',rank_order)\n",
    "                \n",
    "                # print('text:',text,'match:',match)\n",
    "                task_id=data[\"task_id\"]\n",
    "                request_data=df[df['task_id'] == task_id]\n",
    "                origin_pos_line=request_data[\"original_positions_after_shuffle\"].values[0]\n",
    "                order_number = []\n",
    "                for item in order.split(' '):\n",
    "                    if len(item) == 1 and item in ['A','B','C','D','E']:\n",
    "                        order_number.append(ord(item) - ord('A'))\n",
    "                    else:\n",
    "                        continue\n",
    "                        print(f\"item:{item}\")\n",
    "                if len(order_number)>len(origin_pos_line):\n",
    "                    continue\n",
    "                ranked_origin_pos_line=[]\n",
    "                for i in order_number:\n",
    "                    if i<len(origin_pos_line):\n",
    "                        # print(i,'--',len(origin_pos_line))\n",
    "                        ranked_origin_pos_line.append(origin_pos_line[i])\n",
    "                    else:\n",
    "                        print(\"i outOfindex:\",i)\n",
    "                group_id = int(task_id.split(\"-\")[0])\n",
    "                result[group_id] = ranked_origin_pos_line\n",
    "                valid_num+=1\n",
    "        print(f\"valid_num:{valid_num},except_num:{except_num}\")\n",
    "        return result\n",
    "def get_from_multi_files(exps):\n",
    "    # exps=[0,1,2]\n",
    "    all_y_preds={}\n",
    "    for exp in exps:\n",
    "        df = load_jsonl_to_dataframe(f\"{base_folder}/{dataset_name}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/request.jsonl\")\n",
    "\n",
    "        response_path=f\"{data_folder}/{dataset_name}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/response.jsonl\"\n",
    "        y_pred=get_from_file(response_path,df)\n",
    "        for k,v in y_pred.items():\n",
    "            if k not in all_y_preds.keys():\n",
    "                all_y_preds[k]=[v]\n",
    "            else:\n",
    "                all_y_preds[k].append(v)\n",
    "    return all_y_preds\n",
    "    # conbined_y_pred={}\n",
    "\n",
    "def get_each_file_result(data_folder,exps,begin_index,end_index):\n",
    "    ndcg1s = []\n",
    "    for exp in exps:\n",
    "        file_path=f\"{base_folder}/{dataset_name}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/result.jsonl\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                if \"NDCG@1\" in data:\n",
    "                    ndcg1 = data[\"NDCG@1\"]\n",
    "                    ndcg1s.append(ndcg1)\n",
    "                    break\n",
    "    return sum(ndcg1s)/len(ndcg1s),ndcg1s\n",
    "\n",
    "def get_by_min_entropy(content_s, ranked_origin_pos, num=10, save_multi=True):\n",
    "    final_to_combined = {}\n",
    "    used_e = {}\n",
    "    cnt={}\n",
    "    for i in range(num):\n",
    "        cnt[i]=0\n",
    "    # print('ranked_origin_pos:',ranked_origin_pos)\n",
    "    for key, val in content_s.items():\n",
    "        key_ranked_pos = ranked_origin_pos[key]\n",
    "        key_num = int(key.split(\"-\")[0])\n",
    "        \n",
    "        # Find the position of minimum entropy\n",
    "        min_entropy_pos = val.index(min(val))\n",
    "        cnt[min_entropy_pos]+=1\n",
    "        # Save the corresponding model output / posterior probability ranking result\n",
    "        if save_multi:\n",
    "            # Save multiple results\n",
    "            final_to_combined[key_num] = key_ranked_pos[max(0, min_entropy_pos-3) : min_entropy_pos+1]\n",
    "        else:\n",
    "            # Save one result\n",
    "            final_to_combined[key_num] = key_ranked_pos[min_entropy_pos]\n",
    "\n",
    "        used_e[key] = min_entropy_pos\n",
    "    print('min_entropy_index_cnt:',cnt)\n",
    "    return final_to_combined, used_e\n",
    "def copeland_aggregation(to_combined):\n",
    "    \"\"\"\n",
    "    Aggregates multiple ranking lists using the Copeland's method.\n",
    "\n",
    "    :param rankings_list: List of rankings.\n",
    "    :return: Aggregated ranking based on Copeland's method.\n",
    "    \"\"\"\n",
    "    result={}\n",
    "    for key, rankings_list in to_combined.items():\n",
    "        num_items = len(rankings_list[0])  # Get the number of items to rank.\n",
    "        scores = [0] * num_items\n",
    "\n",
    "        # Calculate Copeland scores\n",
    "        for rankings in rankings_list:\n",
    "            for i in range(num_items):\n",
    "                for j in range(i + 1, num_items):\n",
    "                    if rankings.index(i) < rankings.index(j):\n",
    "                        scores[i] += 1\n",
    "                    else:\n",
    "                        scores[j] += 1\n",
    "\n",
    "        # Sort items by Copeland scores\n",
    "        sorted_items = sorted(range(num_items), key=lambda k: -scores[k])\n",
    "        # print(sorted_items,)\n",
    "        result[key] = sorted_items\n",
    "    return result\n",
    "def median_aggregation(to_combined):\n",
    "    \"\"\"\n",
    "    Aggregates multiple ranking lists using the Median method.\n",
    "\n",
    "    :param to_combined: Dictionary where each key has a list of rankings.\n",
    "    :return: A dictionary of aggregated rankings.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    for key, rankings_list in to_combined.items():\n",
    "        num_items = len(rankings_list[0])  # Get the number of items to rank.\n",
    "        median_ranks = {}\n",
    "\n",
    "        # Calculate median rank for each item\n",
    "        for i in range(num_items):\n",
    "            ranks_of_i = [rankings.index(i) for rankings in rankings_list]\n",
    "            sorted_ranks = sorted(ranks_of_i)\n",
    "\n",
    "            # If there's an even number of rankings, take the average of the two middle ranks\n",
    "            if len(sorted_ranks) % 2 == 0:\n",
    "                median_rank = (sorted_ranks[len(sorted_ranks) // 2 - 1] + sorted_ranks[len(sorted_ranks) // 2]) / 2\n",
    "            else:\n",
    "                median_rank = sorted_ranks[len(sorted_ranks) // 2]\n",
    "\n",
    "            median_ranks[i] = median_rank\n",
    "\n",
    "        # Sort items by their median rank\n",
    "        sorted_items = sorted(median_ranks.keys(), key=lambda k: median_ranks[k])\n",
    "        result[key] = sorted_items\n",
    "    return result\n",
    "content_s={}\n",
    "content_post={}\n",
    "content_order={}\n",
    "ranked_origin_pos={}\n",
    "post_ranked_pos={}\n",
    "\n",
    "num=10\n",
    "all_numbers = list(range(20))\n",
    "start_idx=0\n",
    "exps_cands=range(start_idx,start_idx+num) # [0,1,2]\n",
    "\n",
    "exps=[2,3,4]\n",
    "\n",
    "data_folder=base_folder\n",
    "save_multi=True\n",
    "\n",
    "data=pd.read_csv(f\"./data/{dataset_name}/LLM/topk_candidate@5_history@5_origin.csv\",delimiter=\"\\t\")#, usecols=C.DATA_COL_NAME_LIST\n",
    "y_true=get_y_true(data,begin_index,end_index)\n",
    "for k,v in y_true.items():\n",
    "    y_true[k]=[v]\n",
    "\n",
    "for exp in exps_cands:\n",
    "    df = load_jsonl_to_dataframe(f\"{base_folder}/{dataset_name}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/request.jsonl\")\n",
    "    \n",
    "    file_path = f'{base_folder}/{dataset_name}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/response.jsonl'\n",
    "    content_s,content_post,content_order,ranked_origin_pos,post_ranked_pos=cal_s(file_path,content_s,content_post,content_order,ranked_origin_pos,post_ranked_pos,df)  \n",
    "booststrap_y=get_from_multi_files(exps)\n",
    "\n",
    "result_path=f\"{base_folder}/result012.jsonl\"\n",
    "candidate_num=5\n",
    "\n",
    "methods =['entropy','score']# ,'score','single_file' [  'entropy',]#'voting', ,'other'\n",
    "# methods=['other','entropy']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    if method == 'score':\n",
    "        y_pred = conbine_by_score(booststrap_y)\n",
    "    elif method == 'other':\n",
    "        # y_pred = combine_by_voting(booststrap_y)\n",
    "        y_pred=copeland_aggregation(booststrap_y)\n",
    "        \n",
    "        # y_pred=median_aggregation(booststrap_y)\n",
    "    elif method == 'single_file':\n",
    "        avg,each_res=get_each_file_result(data_folder,exps,begin_index,end_index)\n",
    "        #\n",
    "        # y_pred = get_from_file(seleted_file_path)\n",
    "        results[f\"avg_{exps}\"]=round(avg,4)\n",
    "        results[f\"min_{exps}\"]=round(min(each_res),4)\n",
    "        # results[f\"avg_{exps}\"]=avg\n",
    "\n",
    "        results[f\"max_{exps}\"]=round(max(each_res),4)\n",
    "        continue\n",
    "    elif method == 'entropy':\n",
    "        # final_to_conbined,used_e=get_by_e(content_s,ranked_origin_pos,content_post,num=num,save_multi=save_multi)#\n",
    "        final_to_conbined,used_e=get_by_min_entropy(content_s,ranked_origin_pos,num=num,save_multi=save_multi)#\n",
    "        # print(final_to_conbined)\n",
    "        if save_multi:\n",
    "            y_pred = conbine_by_score(final_to_conbined)\n",
    "        else:\n",
    "            y_pred = final_to_conbined\n",
    "    else:\n",
    "        continue\n",
    "    result = Eval.eval_result(\n",
    "        task=\"List\",\n",
    "        result_path=result_path,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        topk=[1,2,3,4,5][:5], # or [1,2,3,4,5,6,7,8,10] based on your needs\n",
    "        candidate_num=candidate_num\n",
    "    )\n",
    "    results[method] = result[3][\"Precision@1\"]\n",
    "print(f\"{dataset_name} exp {num}:\\n\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "methods = ['score','voting','unweighted_entropy','entropy','sc-entropy'] #\n",
    "total_num=20\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "\n",
    "def calculate_metrics(begin_index, end_index, num, start_idx, base_folder, dataset_name):\n",
    "    # 初始化字典\n",
    "    content_s = {}\n",
    "    content_post = {}\n",
    "    content_order = {}\n",
    "    ranked_origin_pos = {}\n",
    "    post_ranked_pos = {}\n",
    "\n",
    "    \n",
    "    exps_cands = range(start_idx, start_idx + num)\n",
    "    exps_cands = list(exps_cands)\n",
    "    # print(f\"exps_cands: {exps_cands}\")\n",
    "    for i, exp in enumerate(exps_cands):\n",
    "        exps_cands[i] = exp%total_num\n",
    "    print(f\"exps_cands: {exps_cands}\")\n",
    "    \n",
    "    data_folder = dataset_name\n",
    "    save_multi = False\n",
    "    origin_data = f\"./data/{dataset_name}/LLM/topk_candidate@5_history@5_origin.csv\"\n",
    "    data = pd.read_csv(origin_data, delimiter=\"\\t\")\n",
    "\n",
    "    y_true = get_y_true(data, begin_index, end_index)\n",
    "    for k, v in y_true.items():\n",
    "        y_true[k] = [v]\n",
    "\n",
    "    for exp in exps_cands:\n",
    "        file_path = f'{base_folder}/{data_folder}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/response.jsonl'\n",
    "        df = load_jsonl_to_dataframe(f\"{base_folder}/{dataset_name}_test/{exp}/gpt-3.5-turbo/idx{begin_index}-{end_index}_posIdx@-1_shuffle_neg_cands@1_example@1_list_candidate@5_history@5/request.jsonl\")\n",
    "\n",
    "        content_s, content_post, content_order, ranked_origin_pos, post_ranked_pos = cal_s(file_path, content_s, content_post, content_order, ranked_origin_pos, post_ranked_pos,df)\n",
    "\n",
    "    booststrap_y = get_from_multi_files(exps_cands)\n",
    "    # print(f\"booststrap_y: {booststrap_y}\")\n",
    "    result_path = f\"{base_folder}/result012.jsonl\"\n",
    "    candidate_num = 5\n",
    "    results = {}\n",
    "    pre_correct={}\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"method: {method}\")\n",
    "        if method == 'entropy':\n",
    "            final_to_conbined, used_e = get_by_min_entropy(content_s, ranked_origin_pos, num=num, save_multi=True)\n",
    "            y_pred = conbine_by_score(final_to_conbined)\n",
    "        if method == 'sc-entropy':\n",
    "            final_to_conbined, used_e = get_by_min_entropy(content_s, ranked_origin_pos, num=num, save_multi=True)\n",
    "            y_pred = combine_by_voting(final_to_conbined)\n",
    "        elif method == 'unweighted_entropy':\n",
    "            final_to_conbined, used_e = get_by_min_entropy(content_s, ranked_origin_pos, num=num, save_multi=False)\n",
    "            y_pred = final_to_conbined\n",
    "        elif method == 'score':\n",
    "            y_pred = conbine_by_score(booststrap_y)\n",
    "        # print('y_pred:\\n', y_pred, '\\ny_true:\\n', y_true)\n",
    "        elif method == 'voting':\n",
    "            y_pred = combine_by_voting(booststrap_y)\n",
    "        result = Eval.eval_result(\n",
    "            task=\"List\",\n",
    "            result_path=result_path,\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            topk=[1,2,3,4,5][:5],  # or [1,2,3,4,5,6,7,8,10] based on your needs\n",
    "            candidate_num=candidate_num\n",
    "        )\n",
    "        print(f\"result: {result}\")\n",
    "        results[method] = result[3][\"Precision@1\"]\n",
    "        pre_correct[method]={}\n",
    "        for k,v in y_pred.items():\n",
    "            if v[0]==y_true[k][0]:\n",
    "                pre_correct[method][k]=1\n",
    "            else:\n",
    "                pre_correct[method][k]=0\n",
    "    print(f\"results: {results}\")\n",
    "    print('='*20)\n",
    "    return results,pre_correct\n",
    "\n",
    "def process_for_start_idx(begin_index, end_index, num, start_idx, base_folder, dataset_name):\n",
    "\n",
    "    results,pre_correct = calculate_metrics(begin_index, end_index, num, start_idx, base_folder, dataset_name)\n",
    "    return results,pre_correct\n",
    "\n",
    "\n",
    "print(dataset_name)\n",
    "start_idx_values = [0,3,6]#[9,4,7]#range(9,total_num,3)  # start_idx 的范围 1\n",
    "print(f\"start_idx_values: {list(start_idx_values)}\")\n",
    "all_results = {}\n",
    "pre_corrects={}\n",
    "\n",
    "for start_idx in start_idx_values:\n",
    "    results,pre_correct = process_for_start_idx(begin_index, end_index, num, start_idx, base_folder, dataset_name)\n",
    "    all_results[start_idx] = results \n",
    "    pre_corrects[start_idx]=pre_correct\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
